task,model_name,score,abstention_rate
lawbench_ZXFL,tigerbot-sft-7b-hf,0.186,0.062
lawbench_ZXFL,gogpt-7b,0.09,0.516
lawbench_ZXFL,chatlaw-13b-hf,0.276,0.158
lawbench_ZXFL,llama-2-chinese-7b-hf,0.002,0.994
lawbench_ZXFL,wizardlm-7b-hf,0.07,0.082
lawbench_ZXFL,vicuna-33b-hf,0.082,0.554
lawbench_ZXFL,vicuna-7b-hf,0.074,0.21
lawbench_ZXFL,yulan-chat-2-13b-fp16-hf,0.338,0.094
lawbench_ZXFL,llama-2-7b-chat,0.052,0.466
lawbench_ZXFL,qwen-7b-chat-hf,0.35,0.13
lawbench_ZXFL,llama-2-70b,0.104,0.22
lawbench_ZXFL,moss-moon-003-base-hf,0.018,0.872
lawbench_ZXFL,llama-2-13b-chat,0.248,0.102
lawbench_ZXFL,chatglm2-6b-hf,0.224,0.248
lawbench_ZXFL,lawyer-llama-13b-hf,0.044,0.786
lawbench_ZXFL,chinese-alpaca-2-7b-hf,0.202,0.164
lawbench_ZXFL,llama-2-70b-chat,0.304,0.014
lawbench_ZXFL,freewilly2_70b-hf,0.39,0.018
lawbench_ZXFL,llama-13b,0.022,0.764
lawbench_ZXFL,baichuan-13b-base-hf,0.036,0.832
lawbench_ZXFL,mpt-7b-hf,0.02,0.786
lawbench_ZXFL,ziya-llama-13b-hf,0.246,0.34
lawbench_ZXFL,qwen-7b-hf,0.248,0.264
lawbench_ZXFL,baichuan-7b-hf,0.028,0.874
lawbench_ZXFL,internlm-chat-7b-8k-hf,0.356,0.078
lawbench_ZXFL,belle-2-13b-hf,0.38,0.02
lawbench_ZXFL,llama-7b,0.032,0.68
lawbench_ZXFL,alpaca-7b-hf,0.038,0.412
lawbench_ZXFL,llama-2-chinese-13b-hf,0.0,0.996
lawbench_ZXFL,GPT-3.5-turbo-0613,0.412,0.034
lawbench_ZXFL,llama-2-7b,0.022,0.836
lawbench_ZXFL,xverse-13b-hf,0.018,0.51
lawbench_ZXFL,llama-65b,0.01,0.95
lawbench_ZXFL,GPT4,0.44,0.016
lawbench_ZXFL,llama-2-13b,0.018,0.956
lawbench_ZXFL,moss-moon-003-sft-hf,0.032,0.1
lawbench_ZXFL,chinese-llama-2-7b-hf,0.016,0.93
lawbench_ZXFL,llama-30b,0.022,0.814
lawbench_ZXFL,internlm-7b-hf,0.02,0.876
lawbench_ZXFL,mpt-instruct-7b-hf,0.108,0.38
lawbench_ZXFL,internlm-chat-7b-hf,0.356,0.078
lawbench_ZXFL,chatlaw-33b-hf,0.198,0.444
lawbench_ZXFL,vicuna-13b-hf,0.112,0.13
lawbench_ZXFL,baichuan-13b-chat-hf,0.026,0.94
lawbench_ZXFL,tigerbot-base-7b-hf,0.222,0.122
